{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_logistic_regression_for_text_mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1uq_7ar9puJJG8cKR8fRE2iwFCEpAq7DQ",
      "authorship_tag": "ABX9TyO+GMrIKHdAdD2mkbHihSWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/courses/blob/master/SML2020/10_logistic_regression_for_text_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77taeHZLUaTm",
        "colab_type": "text"
      },
      "source": [
        "# ロジスティック回帰を使ってテキストマイニングを試みる\n",
        "\n",
        "* WikipediaからUSの男性俳優と女性俳優のページをクローリングして、簡単な前処理をしたデータを使う。\n",
        "\n",
        "* 分析の目的1: 検証データでできるかぎりチューニングを行い、最後にテストデータでの分類性能を明らかにする。\n",
        "\n",
        "* 分析の目的2： 男性俳優と女性俳優のページを分類する際に、どのような単語が特に効いているかを調べる。\n",
        "\n",
        " * この調査によって、俳優に関する記述におけるジェンダー・ステレオタイプを明らかにできるか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qos6P0RsSsqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEMUit1zVZma",
        "colab_type": "text"
      },
      "source": [
        "### データファイルを読み込む\n",
        "\n",
        "* データファイルは、あらかじめ自分のGoogle Driveの適当なフォルダに置いておく。\n",
        "\n",
        "* データファイルの各行には、女性俳優(1)か男性俳優(0)かを表すフラグ、俳優の名前、Wikipediaのページの本文が、この順に格納されている。\n",
        "\n",
        "* データファイルの各行に対してeval組み込み関数を適用すると、Pythonのリストに変換できるようなフォーマットで、ファイルに記録されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngsjTPa8TqgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = list()\n",
        "names = list()\n",
        "corpus = list()\n",
        "with open('drive/My Drive/data/us_actors_and_actresses.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        flag, name, text = eval(line.strip())\n",
        "        y.append(int(flag))\n",
        "        names.append(name)\n",
        "        corpus.append(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB1KVAZ6YB3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(y)\n",
        "names = np.array(names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR0Ew63dY-1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51c79f23-a4fd-48a2-eb35-872b8d4417ed"
      },
      "source": [
        "N = len(y)\n",
        "print('We have {:d} documents.'.format(N))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 19645 documents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqhGQYYQbDx-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99946cb4-5244-4435-e208-bc732fe5d561"
      },
      "source": [
        "indices = np.arange(N)\n",
        "np.random.seed(123)\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12617 18144 17561 ... 15377 17730 15725]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuNIHWRbbMC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68e3d174-384b-4f60-c74a-026a399c2bb4"
      },
      "source": [
        "border = N * 8 // 10\n",
        "print('We have {:d} test documents.'.format(N - border))\n",
        "train_indices = indices[:border]\n",
        "test_indices = indices[border:]\n",
        "y_train = y[train_indices]\n",
        "y_test = y[test_indices]\n",
        "names_train = names[train_indices]\n",
        "names_test = names[test_indices]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 3929 test documents.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPZeCpZlesne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_train = list()\n",
        "for i in train_indices:\n",
        "  corpus_train.append(corpus[i])\n",
        "corpus_test = list()\n",
        "for i in test_indices:\n",
        "  corpus_test.append(corpus[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZFZGWwte8fu",
        "colab_type": "text"
      },
      "source": [
        "（ここまでのtraining setとtest setへの分割は、変えないようにしてください。）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKg-InjbVh7r",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDFで各文書をベクトル化する\n",
        "\n",
        "* TF-IDFは単語列をベクトル化する方法のひとつ。\n",
        "\n",
        "* ベクトルの次元は語彙数となる。各Wikipediaのページがひとつのベクトルへ変換される。\n",
        "\n",
        "* TfidfVectorizerのパラメータをチューニングしても構わない\n",
        "\n",
        " * ここでTF-IDFの計算をするときにテストデータは使っていないので、ズルはしていない。\n",
        "\n",
        " * min_dfは、その数より少ない文書にしか出現しない単語を削除する、という意味のパラメータ。希少な単語を削除するために使う。\n",
        "\n",
        " * max_dfは、0から1の間の実数で指定すると、その割合より多い文書に出現する単語を削除する、という意味のパラメータ。ありふれた単語を削除するために使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsek-uzTxi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e95ba048-230f-45b5-c855-e288535951fd"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=100)\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "print('# X_train shape', X_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# X_train shape (15716, 6033)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcTnazzT5Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.array(vectorizer.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e2QrURgT_gr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6ac619b8-f1ab-4e77-ecfa-f4f90888d9a6"
      },
      "source": [
        "print(vocab[1000:1010])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bringing' 'brings' 'britain' 'british' 'broad' 'broadcast'\n",
            " 'broadcasting' 'broadcasts' 'broadway' 'broderick']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCHxuAIwWUQc",
        "colab_type": "text"
      },
      "source": [
        "###  交差検証の準備\n",
        "\n",
        "* ここはk-fold交差検証に書き換えても良い。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHPN9nvfUCwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.10, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmRAzGE1W4af",
        "colab_type": "text"
      },
      "source": [
        "### ロジスティック回帰の学習\n",
        "\n",
        "* ロジスティック回帰のハイパーパラメータは、検証データでの分類性能ができるだけよくなるように、変更する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982vmj3KUH-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03415802-5d48-40ab-87fe-15881bed1697"
      },
      "source": [
        "# これは単純な実行例にすぎません。正則化も含めてチューニングしてください。\n",
        "# TfidfVectorizerのパラメータも併せてチューニングしていいです。\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear', random_state=123)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.score(X_valid, y_valid))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8829516539440203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWcZHyJXFzR",
        "colab_type": "text"
      },
      "source": [
        "### テストデータで最終的な評価をおこなう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQE3CxUdfZ7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "596222c4-9248-4f17-d0fe-66afeb51896c"
      },
      "source": [
        "X_test = vectorizer.transform(corpus_test)\n",
        "print('# X_test shape', X_test.shape)\n",
        "print(clf.score(X_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# X_test shape (3929, 6033)\n",
            "0.8788495800458132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CtVT3KEXM7f",
        "colab_type": "text"
      },
      "source": [
        "### 分類に効いている単語を調べる\n",
        "\n",
        "* 訓練データが最も数が多いので、訓練データの分類に最も効いている単語100語を調べる。\n",
        "\n",
        "* 下に示すのは、あくまで一つの方法にすぎない。他にどんな方法があるか調べて、その方法を実践する。\n",
        "\n",
        " * 下の手法の欠点は、男性俳優の文書に特徴的な単語と、女性俳優の文書に特徴的な単語とを、区別できない点である。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7An3q7JUK8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6b223b26-c60e-4d00-a258-5905659c9b7b"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "rfe = RFE(estimator=clf, n_features_to_select=200, step=100)\n",
        "rfe.fit(X_train, y_train)\n",
        "ranking = rfe.ranking_\n",
        "print(vocab[ranking == 1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['actresses' 'adam' 'age' 'alice' 'amanda' 'amy' 'andrew' 'angela' 'anime'\n",
            " 'ann' 'anna' 'anne' 'annie' 'announcer' 'anthony' 'army' 'attack' 'aunt'\n",
            " 'baby' 'band' 'baseball' 'beauty' 'ben' 'billy' 'birth' 'blonde' 'bobby'\n",
            " 'boy' 'boys' 'breast' 'brother' 'brothers' 'captain' 'catherine'\n",
            " 'character' 'charles' 'cheerleader' 'chris' 'christopher' 'cindy'\n",
            " 'claire' 'contract' 'corps' 'dance' 'dancer' 'daniel' 'daughter' 'dave'\n",
            " 'david' 'debut' 'detective' 'died' 'directed' 'directing' 'director'\n",
            " 'divorced' 'dorothy' 'eddie' 'edward' 'elizabeth' 'emily' 'eric' 'eve'\n",
            " 'fashion' 'father' 'female' 'florence' 'football' 'frances' 'frank'\n",
            " 'fraternity' 'game' 'gang' 'gay' 'george' 'gina' 'girl' 'girlfriend'\n",
            " 'girls' 'grace' 'grandmother' 'guitar' 'guy' 'heather' 'helen' 'hero'\n",
            " 'heroine' 'husband' 'ii' 'irene' 'jack' 'james' 'jane' 'jason' 'jay'\n",
            " 'jean' 'jeff' 'jennifer' 'jenny' 'jessica' 'jim' 'jimmy' 'joe' 'john'\n",
            " 'johnny' 'jonathan' 'joseph' 'joy' 'jr' 'julie' 'katie' 'kevin' 'lady'\n",
            " 'larry' 'laura' 'lesbian' 'lifetime' 'linda' 'lisa' 'louise' 'love'\n",
            " 'lynn' 'magazine' 'maggie' 'maid' 'male' 'man' 'margaret' 'maria' 'marie'\n",
            " 'marriage' 'married' 'martial' 'mary' 'men' 'michael' 'michelle' 'mike'\n",
            " 'military' 'miss' 'model' 'modeling' 'molly' 'mother' 'mr' 'mrs' 'ms'\n",
            " 'nancy' 'navy' 'nicole' 'niece' 'nurse' 'officer' 'opposite' 'paul'\n",
            " 'penny' 'performance' 'peter' 'playboy' 'police' 'pregnant' 'princess'\n",
            " 'queen' 'rapper' 'rebecca' 'retired' 'richard' 'robert' 'rose' 'ruth'\n",
            " 'sally' 'sam' 'samantha' 'sandra' 'sarah' 'sean' 'served' 'sheriff'\n",
            " 'singer' 'singing' 'sister' 'sisters' 'son' 'sorority' 'stand' 'stuntman'\n",
            " 'sue' 'susan' 'team' 'tommy' 'victoria' 'villain' 'waitress' 'war'\n",
            " 'wendy' 'wife' 'william' 'woman' 'women' 'wrote']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuJHw6ZfTRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
